{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4\n",
    "\n",
    "def linklist(tag):\n",
    "    link = tag.find('a')['href']\n",
    "    return link\n",
    "\n",
    "f = open(\"linklist.txt\",\"w\")\n",
    "\n",
    "siteurl = \"https://news.daum.net/\"\n",
    "htmlObject = urllib.request.urlopen(siteurl)\n",
    "webPage = htmlObject.read()\n",
    "bsObject = bs4.BeautifulSoup(webPage, 'html.parser')\n",
    "tag = bsObject.find('ul',{'class':'list_newsissue'})\n",
    "li_list = tag.findAll('li')\n",
    "for li in li_list:\n",
    "    f.write(linklist(li)+'\\n')    \n",
    "f.close()\n",
    "\n",
    "\n",
    "f1 = open(\"article.txt\",\"w\")\n",
    "for li in li_list:\n",
    "    siteurl = linklist(li)\n",
    "    htmlObject = urllib.request.urlopen(siteurl)\n",
    "    webPage = htmlObject.read()\n",
    "    bsObject = bs4.BeautifulSoup(webPage, 'html.parser')\n",
    "    tag = bsObject.find('div',{'class':'article_view'})\n",
    "    txt = tag.findAll('p',{'dmcf-ptype':'general'})\n",
    "    sub = bsObject.find('h3',{'class':'tit_view'}).text\n",
    "    #f1.write(\"기사 제목 :\"+sub+\"\\n\\n\")\n",
    "    #f1.write(\"기사 링크 :\"+linklist(li)+\"\\n\\n\")\n",
    "    for text in txt:\n",
    "        f1.write(text.text+'\\n')\n",
    "    f1.write('\\n')\n",
    "    \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
